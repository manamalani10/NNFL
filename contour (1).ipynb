{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: The paging file is too small for this operation to complete.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e5c10665d608>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtvf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\models\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0malexnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mvgg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0msqueezenet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0minception\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\models\\alexnet.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_state_dict_from_url\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0m_dl_flags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The paging file is too small for this operation to complete."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torchvision.transforms import functional as tvf\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import random\n",
    "import scipy.ndimage.filters\n",
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "from multiprocessing import Queue \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ugv8OW73bKlR"
   },
   "outputs": [],
   "source": [
    "paths = {\n",
    "\"images_path\" : \"drive/My Drive/NNFL_project/VOC2012/JPEGImages\",\n",
    "\"targets_path\" : \"drive/My Drive/NNFL_project/improved_contour\",\n",
    "\"train_names_path\" : \"drive/My Drive/NNFL_project/VOC2012/ImageSets/Segmentation/train.txt\",\n",
    "\"val_names_path\" : \"drive/My Drive/NNFL_project/VOC2012/ImageSets/Segmentation/val.txt\",\n",
    "\"model_save_path\": \"drive/My Drive/NNFL_project/models\"\n",
    "}\n",
    "\n",
    "for p in paths.values():\n",
    "    if(os.path.exists(p) == False):\n",
    "        print(\"path \" , p , \"does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68787,
     "status": "error",
     "timestamp": 1587152903219,
     "user": {
      "displayName": "RUSHI JAYESHKUMAR BABARIYA",
      "photoUrl": "",
      "userId": "09653925580240235591"
     },
     "user_tz": -330
    },
    "id": "rdTYcrBebH0R",
    "outputId": "dc9a5f98-bf86-497e-fbb1-fa7bd458ebb0"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-74e2d9272fcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-74e2d9272fcd>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, paths)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_images_pil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_tar_pil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_val_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-74e2d9272fcd>\u001b[0m in \u001b[0;36mmake_train_val_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mval_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mclassaug_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"targets_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mval_set\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassaug_imgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: 'drive/My Drive/NNFL_project/improved_contour'"
     ]
    }
   ],
   "source": [
    "class dataloader:\n",
    "    def __init__(self,paths):\n",
    "        self.paths = paths\n",
    "        self.train_images_pil = []\n",
    "        self.train_tar_pil = []\n",
    "        self.make_train_val_names()\n",
    "        self.pointer = 0\n",
    "        self.index_arr = [x for x in range(len(self.train_names))]\n",
    "\n",
    "        \n",
    "    def make_train_val_names(self):\n",
    "        with open(paths[\"train_names_path\"]) as handle:\n",
    "            orignal_train_names = [x.split(\"\\n\")[0].strip() for x in handle]\n",
    "        with open(paths[\"train_names_path\"]) as handle:\n",
    "            val_names = [x.split(\"\\n\")[0].strip() for x in handle]\n",
    "        self.train_names = []\n",
    "        classaug_imgs = os.listdir(self.paths[\"targets_path\"])\n",
    "        val_set =set(val_names)\n",
    "        for name in classaug_imgs:\n",
    "            name = name.split(\".\")[0]\n",
    "            if name not in val_set:\n",
    "                self.train_names.append(name)\n",
    "        self.train_names += orignal_train_names\n",
    "\n",
    "        self.train_names = list(set(self.train_names))\n",
    "        self.val_names = list(val_set)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_names)\n",
    "\n",
    "    def reset_loader(self):\n",
    "        random.shuffle(self.index_arr)\n",
    "        self.pointer = 0\n",
    "\n",
    "    def transform(self, image_origin, mask_origin, mode, data_augmentation = \"randomcrop\"):\n",
    "        image_res, mask_res = None, None\n",
    "        totensor_op = transforms.ToTensor()\n",
    "        color_op = transforms.ColorJitter(0.1, 0.1, 0.1)\n",
    "        resize_op = transforms.Resize((224, 224))\n",
    "        image_origin = color_op(image_origin)\n",
    "      #  norm_op = transforms.Normalize(mean =[0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "\n",
    "        if mode == 'val' or mode == 'predict':\n",
    "            image_res = totensor_op(image_origin)\n",
    "            mask_res = totensor_op(mask_origin)\n",
    "        elif mode == 'train':\n",
    "            if data_augmentation == 'randomcrop':\n",
    "                if image_origin.size[0] < 224 or image_origin.size[1] < 224:\n",
    "                    #padding-val:\n",
    "                    val = int(np.array(image_origin).sum() / image_origin.size[0] / image_origin.size[1])\n",
    "                    padding_width = 224-min(image_origin.size[0],image_origin.size[1])\n",
    "                    padding_op = transforms.Pad(padding_width,fill=val)\n",
    "                    image_origin = padding_op(image_origin)\n",
    "                    padding_op = transforms.Pad(padding_width, fill=0)\n",
    "                    mask_origin = padding_op(mask_origin)\n",
    "                i, j, h, w = transforms.RandomCrop.get_params(\n",
    "                    image_origin, output_size=(224, 224)\n",
    "                )\n",
    "                image_res = totensor_op(tvf.crop(image_origin, i, j, h, w))\n",
    "                mask_res = totensor_op(tvf.crop(mask_origin, i, j, h, w))\n",
    "\n",
    "            elif data_augmentation == 'resize':\n",
    "                image_res = totensor_op(resize_op(image_origin))\n",
    "                mask_res = totensor_op(resize_op(mask_origin))\n",
    "      #  image_res = norm_op(image_res)\n",
    "     \n",
    "        return image_res, mask_res\n",
    "\n",
    "\n",
    "    def _make_dicts(self):\n",
    "        count= 0\n",
    "        for img_name in self.train_names:\n",
    "            count += 1\n",
    "            print(count)\n",
    "            img_path = os.path.join(self.paths[\"images_path\"] , img_name + \".jpg\")\n",
    "            mask_path = os.path.join(self.paths[\"targets_path\"] , img_name + \".png\")\n",
    "            img = Image.open(img_path)\n",
    "            tar = Image.open(mask_path)\n",
    "            self.train_images_pil.append(img)\n",
    "            self.train_tar_pil.append(tar)\n",
    "        \n",
    "    def fill_gap(self,image,filter_size):\n",
    "        new_img = np.zeros((image.shape[0], image.shape[1]), dtype = image.dtype)\n",
    "        img_filter = np.ones((filter_size, filter_size),dtype = image.dtype)\n",
    "        for i in range(0,image.shape[0] -filter_size):\n",
    "            for j in range(0,image.shape[1] - filter_size):\n",
    "                if((image[i:i+filter_size, j:j+ filter_size]*img_filter).sum() > 2):\n",
    "                    new_img[i:i+filter_size, j: j+ filter_size] = 1\n",
    "        new_img[image == 1] = 1\n",
    "        return new_img\n",
    "    \n",
    "    \n",
    "    def get_next_mini_batch(self, index_given= None):\n",
    "            if(self.pointer == len(self.train_names)):\n",
    "                self.reset_loader()\n",
    "            if(index_given == None):\n",
    "                index = self.pointer\n",
    "            else:\n",
    "                index = index_given\n",
    "            self.pointer += 1\n",
    "            img_name = self.train_names[index]\n",
    "            img_path = os.path.join(self.paths[\"images_path\"] , img_name + \".jpg\")\n",
    "            mask_path = os.path.join(self.paths[\"targets_path\"] , img_name + \".png\")\n",
    "            img = Image.open(img_path)\n",
    "            tar = cv2.imread(mask_path,0)\n",
    "            tar = Image.fromarray(tar)\n",
    "            img_batch = torch.empty(8,3,224,224)\n",
    "            tar_batch = torch.empty(8,1,224,224)\n",
    "\n",
    "            for i in range(4):\n",
    "                img_batch[i] , tar_batch[i] = self.transform(img,tar,\"train\")\n",
    "            img = tvf.hflip(img)\n",
    "            tar = tvf.hflip(tar)\n",
    "\n",
    "            for i in range(4,8):\n",
    "                img_batch[i] , tar_batch[i] = self.transform(img,tar,\"train\")\n",
    "\n",
    "           # tar_batch[tar_batch > .9] = 1\n",
    "           # tar_batch[tar_batch <= .9] = 0\n",
    "            return img_batch , tar_batch\n",
    "            \n",
    "            \n",
    "    def get_next_batch(self, batch_size):\n",
    "        index = np.random.randint(0, len(self.train_names), size = batch_size)\n",
    "        img_batch = torch.empty(batch_size,3,224,224)\n",
    "        tar_batch = torch.empty(batch_size,1,224,224)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            img_name = self.train_names[index[i]]\n",
    "            img_path = os.path.join(self.paths[\"images_path\"] , img_name + \".jpg\")\n",
    "            mask_path = os.path.join(self.paths[\"targets_path\"] , img_name + \".png\")\n",
    "            img = Image.open(img_path)\n",
    "            tar = cv2.imread(mask_path,0)\n",
    "            tar = Image.fromarray(tar)\n",
    "            \n",
    "            if(random.random() > .5):\n",
    "                img = tvf.hflip(img)\n",
    "                tar = tvf.hflip(tar)\n",
    "            img_batch[i] , tar_batch[i] =  self.transform(img,tar,\"train\")\n",
    "\n",
    "        return img_batch, tar_batch\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "d = dataloader(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wmJI-R64Cyb"
   },
   "outputs": [],
   "source": [
    "vgg16 =  torchvision.models.vgg16(pretrained=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFDZJ3pJ4XKw"
   },
   "outputs": [],
   "source": [
    "# Extraction till the first fully connected layer of vgg16 \n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self ,x):\n",
    "        return x.view(x.size(0),-1)\n",
    "\n",
    "\n",
    "vgg = list(vgg16.children())[0]\n",
    "\"\"\"list(vgg16.children())[1] ,\"\"\"\n",
    "vgg = nn.Sequential(*vgg,  Reshape() ,list(vgg16.children())[2][0] )\n",
    "#for p in vgg.parameters():\n",
    "#    p.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IkxDwR5KdA3O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "1 ReLU(inplace=True)\n",
      "2 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "3 ReLU(inplace=True)\n",
      "4 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "5 Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "6 ReLU(inplace=True)\n",
      "7 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "8 ReLU(inplace=True)\n",
      "9 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "10 Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "11 ReLU(inplace=True)\n",
      "12 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "13 ReLU(inplace=True)\n",
      "14 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "15 ReLU(inplace=True)\n",
      "16 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "17 Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "18 ReLU(inplace=True)\n",
      "19 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "20 ReLU(inplace=True)\n",
      "21 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "22 ReLU(inplace=True)\n",
      "23 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "24 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "25 ReLU(inplace=True)\n",
      "26 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "27 ReLU(inplace=True)\n",
      "28 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "29 ReLU(inplace=True)\n",
      "30 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "31 Reshape()\n",
      "32 Linear(in_features=25088, out_features=4096, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# This gives us these \n",
    "for i,name in enumerate(list(vgg.children())):\n",
    "    print(i,name)\n",
    "# Notice that we need Maxpool2d to return indices eevery time in order for the network to perform unpooling\n",
    "# So we will judt jo this manually !!!! :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1EsO2_gBrG_"
   },
   "outputs": [],
   "source": [
    "def plot_hist(tensor):\n",
    "    tensor = tensor.cpu().numpy()\n",
    "    tensor = tensor[(tensor <= .5) & (tensor >= -.5) ]\n",
    "    plt.hist(tensor.reshape(-1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4EudNvQH7bUm"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vgg):\n",
    "        super().__init__()\n",
    "        self.vgg = list(vgg.children())\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "    def forward(self,x):\n",
    "\n",
    "        pooling_info = {}\n",
    "        layer_info = {}\n",
    "        # Starting conv1\n",
    "        x = self.vgg[0](x)\n",
    " \n",
    "        x = self.vgg[1](x)\n",
    "        x = self.vgg[2](x)\n",
    "        x = self.vgg[3](x)\n",
    "        shape = x.shape\n",
    "        \n",
    "        layer_info[1] = {\"value\": x}\n",
    "        x , ind = self.pool1(x)\n",
    "        pooling_info[1] = {\"kernel_size\" : 2, \"stride\": 2, \"padding\": 0 ,\"output_size\": shape,\"indices\":ind}\n",
    "        \n",
    "\n",
    "        # start conv2\n",
    "        x = self.vgg[5](x)\n",
    "        x = self.vgg[6](x)\n",
    "        x = self.vgg[7](x)\n",
    "        x = self.vgg[8](x)\n",
    "\n",
    "        shape = x.shape\n",
    "        layer_info[2] = {\"value\": x}\n",
    "        x , ind = self.pool2(x)\n",
    "        pooling_info[2] = {\"kernel_size\" : 2, \"stride\": 2, \"padding\": 0 ,\"output_size\": shape,\"indices\":ind}\n",
    "\n",
    "\n",
    "\n",
    "        # start conv3\n",
    "        x = self.vgg[10](x)\n",
    "        x = self.vgg[11](x)\n",
    "        x = self.vgg[12](x)\n",
    "        x = self.vgg[13](x)\n",
    "        x = self.vgg[14](x)\n",
    "        x = self.vgg[15](x)\n",
    "\n",
    "        shape = x.shape\n",
    "        layer_info[3] = {\"value\": x}\n",
    "        x , ind = self.pool3(x)\n",
    "        pooling_info[3] = {\"kernel_size\" : 2, \"stride\": 2, \"padding\": 0 ,\"output_size\": shape,\"indices\":ind}\n",
    "  \n",
    "\n",
    "        x = self.vgg[17](x)\n",
    "        x = self.vgg[18](x)\n",
    "        x = self.vgg[19](x)\n",
    "        x = self.vgg[20](x)\n",
    "        x = self.vgg[21](x)\n",
    "        x = self.vgg[22](x)\n",
    "\n",
    "\n",
    "        shape = x.shape\n",
    "        layer_info[4] = {\"value\": x}\n",
    "        x , ind = self.pool4(x)\n",
    "        pooling_info[4] = {\"kernel_size\" : 2, \"stride\": 2, \"padding\": 0 ,\"output_size\": shape,\"indices\":ind}\n",
    "      \n",
    "\n",
    "\n",
    "        x = self.vgg[24](x)\n",
    "        x = self.vgg[25](x)\n",
    "        x = self.vgg[26](x)\n",
    "        x = self.vgg[27](x)\n",
    "        x = self.vgg[28](x)\n",
    "        x = self.vgg[29](x)\n",
    "\n",
    "        shape = x.shape\n",
    "        layer_info[5] = {\"value\": x}\n",
    "        x , ind = self.pool5(x)\n",
    "        pooling_info[5] = {\"kernel_size\" : 2, \"stride\": 2, \"padding\": 0 ,\"output_size\": shape,\"indices\":ind}\n",
    " \n",
    "        x = self.vgg[31](x)\n",
    "        x = self.vgg[32](x)\n",
    "        \n",
    "        return x.unsqueeze(-1).unsqueeze(-1) , pooling_info, layer_info\n",
    "         \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDC0-Ww0qSrx"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b7da6c412b83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m# torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv6\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4096\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.conv6 = nn.Conv2d(in_channels = 4096, out_channels = 512, kernel_size = 1, stride=1)\n",
    "        self.deconv6 = nn.ConvTranspose2d(in_channels = 512, out_channels = 512, kernel_size = 7)\n",
    "        self.deconv5 = nn.ConvTranspose2d(in_channels = 512, out_channels = 512, kernel_size = 5, padding =2)\n",
    "        self.deconv4 = nn.ConvTranspose2d(in_channels = 512, out_channels = 256, kernel_size = 5 , padding = 2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(in_channels = 256, out_channels = 128, kernel_size = 5 ,padding = 2)\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels = 128, out_channels = 64, kernel_size = 5 , padding = 2)\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels = 64, out_channels = 32, kernel_size = 5 ,padding = 2)\n",
    "        self.pred = nn.ConvTranspose2d(in_channels = 32, out_channels = 1, kernel_size = 5, padding = 2)\n",
    "\n",
    "    def forward(self,encoder_out):\n",
    "        x = encoder_out[0]\n",
    "        dicts = encoder_out[1]\n",
    "        x = self.conv6(x)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = self.deconv6(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.max_unpool2d(x, **dicts[5])\n",
    "\n",
    "        \n",
    "        x = self.deconv5(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.max_unpool2d(x, **dicts[4])\n",
    "\n",
    "\n",
    "        x = self.deconv4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.max_unpool2d(x, **dicts[3])\n",
    "\n",
    "\n",
    "        x = self.deconv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.max_unpool2d(x, **dicts[2])\n",
    "\n",
    "\n",
    "        x = self.deconv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.max_unpool2d(x, **dicts[1])\n",
    "        x = self.deconv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = self.pred(x)\n",
    "\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jECTLtX0_nqE"
   },
   "outputs": [],
   "source": [
    "class U_block(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(in_channels = in_channels, out_channels = in_channels, kernel_size = 2,stride = 2)\n",
    "        self.conv1 = nn.Conv2d(in_channels = in_channels, out_channels = in_channels,kernel_size = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = in_channels, out_channels = out_channels,kernel_size = 2,padding = 1)\n",
    "\n",
    "    def forward(self,x, add  = None):\n",
    "        x = self.upsample(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        if(add != None):\n",
    "            x += add\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return nn.functional.relu(x)\n",
    "\n",
    "class Decoder1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.conv1 = nn.Conv2d(in_channels = 4096, out_channels = 512, kernel_size = 1, stride=1)\n",
    "        # size is (bs,512,1,1)\n",
    "        self.upsample1 = nn.ConvTranspose2d(in_channels = 512, out_channels = 512, kernel_size = 7)\n",
    "        # size is (bs,512,7,7)\n",
    "        self.U_block1 = U_block(512,512) # add layer 5\n",
    "        # size is (bs,512,14,14)\n",
    "        self.U_block2 = U_block(512,256)  # add layer 4\n",
    "        # size is (bs,512,28,28)\n",
    "        self.U_block3 = U_block(256,128) # add layer 3\n",
    "        # size i (bs,128,56,56)\n",
    "\n",
    "        self.U_block4 = U_block(128,64) # add layer 2\n",
    "        # size is (bs,64,122,122)\n",
    "\n",
    "        self.U_block5 = U_block(64,32) # add layer 1\n",
    "        # size is (bs,32,224,224)\n",
    "        self.pred = nn.Conv2d(in_channels = 32, out_channels = 1, kernel_size = 3, padding = 1)\n",
    "\n",
    "    def forward(self,encoder_out):\n",
    "        x = encoder_out[0]\n",
    "        layer_info = encoder_out[2]\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.upsample1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = self.U_block1(x,layer_info[5][\"value\"])\n",
    "        x = self.U_block2(x,layer_info[4][\"value\"])\n",
    "        x = self.U_block3(x,layer_info[3][\"value\"])\n",
    "        x = self.U_block4(x,layer_info[2][\"value\"])\n",
    "        x = self.U_block5(x,layer_info[1][\"value\"])\n",
    "\n",
    "        x = self.pred(x)\n",
    "\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eiZQGqNaQrIz"
   },
   "outputs": [],
   "source": [
    "class countour_detector(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(backbone).to(device)\n",
    "        self.decoder = Decoder1().to(device)\n",
    "\n",
    "    def forward(self,x):\n",
    "      #  with torch.no_grad():\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-8tRetZ9aAN"
   },
   "outputs": [],
   "source": [
    "model = countour_detector(vgg)\n",
    "model.load_state_dict(torch.load(os.path.join(paths[\"model_save_path\"],\"model1.pth\")))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "axobsUDmwwmC"
   },
   "outputs": [],
   "source": [
    "class trainer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.train()\n",
    "        self.lr = 1e-4\n",
    "        self.optimizer = torch.optim.Adam([x for x in list(self.model.parameters()) if x.requires_grad == True], lr=self.lr)\n",
    "        self.bce =  nn.BCELoss(reduction = \"none\")\n",
    "        self.mse = nn.MSELoss(reduction = \"none\")\n",
    "        self.loss_array = []\n",
    "\n",
    "\n",
    "    def loss(self,outputs, targets):\n",
    "        mask = (targets == 1)\n",
    "        remask = ~mask\n",
    "\n",
    "        all_loss = self.bce(outputs, targets)\n",
    "        total_dims = 1\n",
    "        for i in range(len(targets.shape)):\n",
    "            total_dims*= targets.size(i)\n",
    "        return (10*all_loss[mask].sum() + all_loss[remask].sum())/total_dims\n",
    "\n",
    "    def train(self, data):\n",
    "            total_loss = 0\n",
    "            images, targets = data[0], data[1]\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            pred = self.model(images)\n",
    "            loss = self.loss(pred , targets)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            print(\"loss = \",loss.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1WgEw9QZLQct"
   },
   "outputs": [],
   "source": [
    "T = trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LNJYFBZesVzA"
   },
   "outputs": [],
   "source": [
    "Q = Queue()\n",
    "def producer(times, dataobject, Q, batch_size = None):\n",
    "    while(times > 0):\n",
    "        while(Q.qsize() > 100):\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        index = random.randint(0, len(dataobject) - 1)\n",
    "        Q.put(dataobject.get_next_mini_batch(index))\n",
    "        times -= 1\n",
    "    Q.put(\"End\")\n",
    "    time.sleep(50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ybpq7fKsTEr"
   },
   "outputs": [],
   "source": [
    "class multiprocess_control:\n",
    "    def __init__(self, dataobject, workers, produce_func, shared_Q, epochs_desired, train_object):\n",
    "        self.dataobject = dataobject\n",
    "        self.workers = workers\n",
    "        self.produce_func = produce_func\n",
    "        self.shared_Q = shared_Q\n",
    "        self.desired_epochs = epochs_desired\n",
    "        self.train_object = train_object\n",
    "\n",
    "    def spawn_producers(self, batch_size= None):\n",
    "        times = len(self.dataobject)*self.desired_epochs\n",
    "        if(times < self.workers):\n",
    "            print(\"too many workers\")\n",
    "            return None\n",
    "        produce_arr =  []\n",
    "        \n",
    "        for i in range(self.workers):\n",
    "            if(i == self.workers - 1):\n",
    "                reps = times//self.workers + self.desired_epochs%self.workers\n",
    "            else:\n",
    "                reps = times//self.workers\n",
    "            produce_arr.append(Process(target = self.produce_func, args = (reps  ,self.dataobject,self.shared_Q,batch_size,)))\n",
    "        \n",
    "        return produce_arr\n",
    "\n",
    "    \n",
    "    def start_training(self, batch_size= None):\n",
    "        arr = self.spawn_producers(batch_size)\n",
    "        for p in arr:\n",
    "            p.start()\n",
    "        time.sleep(1)\n",
    "        end_count = 0\n",
    "        iters = 0\n",
    "        \n",
    "        while(end_count < self.workers):\n",
    "            if(self.shared_Q.empty()):\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            else:\n",
    "                data = Q.get()\n",
    "                if(data == \"End\"):\n",
    "                    print(\"Hello\")\n",
    "                    end_count += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    iters += 1\n",
    "                    if(iters%len(self.dataobject) == 0):\n",
    "                        print(\"Model saved\")\n",
    "                        model = model.to(device)\n",
    "                        torch.save(model.cpu().state_dict(), os.path.join(paths[\"model_save_path\"] , \"model\" + str(iters//len(self.dataobject)) + \".pth\"))\n",
    "                    print(iters, end = \" \")\n",
    "                    self.train_object.train(data)\n",
    "        \n",
    "\n",
    "        for p in arr:\n",
    "            p.join()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i26Jbt04FjmA"
   },
   "outputs": [],
   "source": [
    "multi = multiprocess_control(d,workers = 50,produce_func = producer, shared_Q = Q,epochs_desired = 29,train_object =T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S1KgLffyFyG7"
   },
   "outputs": [],
   "source": [
    "multi.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVsVD3DX0YQe"
   },
   "outputs": [],
   "source": [
    "a,b = d.get_next_batch(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYtoNbAUgxr7"
   },
   "outputs": [],
   "source": [
    "bb = model(a[0:1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "joEAaV4Yg6rn"
   },
   "outputs": [],
   "source": [
    "bb = bb.detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1RD-_-sAhDfb"
   },
   "outputs": [],
   "source": [
    "bb = bb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aRRcK8TNeNkZ"
   },
   "outputs": [],
   "source": [
    "a =  a[0].numpy()\n",
    "a = np.rollaxis(a,2)\n",
    "a = np.rollaxis(a,2)\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T7jbdCcWgmtF"
   },
   "outputs": [],
   "source": [
    "bb[bb > .5] = 1\n",
    "bb[bb <= .5] = 0\n",
    "plt.imshow(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n5N7de7-eOXR"
   },
   "outputs": [],
   "source": [
    "b = b[0][0].numpy()\n",
    "plt.imshow(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oflVX8tXsQaz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMtLFa5cnCHXZz52Q7CwWyF",
   "collapsed_sections": [],
   "mount_file_id": "1NzM9tSQT2wjsp7sN9hgi4Z8Tbh_8yXYK",
   "name": "contour.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
